{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <h1>Carolyne Pelletier 101054962</h1></div>\n",
    "<div style=\"text-align: right\"> <h1>Akhil Dalal 100855466</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a feed forward neural network in python that classifies the images found in the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed forward neural network consists of three layers. Each layer consists of a certain number of neurons. Note that this is a fully connected network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer takes in the greyscale values of each pixel in a 28x28 pixel image of a handwritten number. Each neuron takes one pixel as input, therefore, there are 784 neurons in the input layer.  \n",
    "The pixel greyscale values range from 0 to 255. We normalize this to be between 0.0 and 1.0 by dividing each pixel's value by 255. This prevents any overflow when we use the sigmoid activation function.  \n",
    "The input layer sends a weighted sum of its input to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be multiple layers in the hidden layers. In our algorithm, we chose just a single hidden layer with 30 neurons.  \n",
    "This layer gets the input from the input layer, and feeds it through the sigmoid activation function to get an output.  \n",
    "The input for the next layer is the weighted sum of the output from the current layer with the weights to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer gets its inputs from the hidden layer. It puts the input through the sigmoid activation function to give the output for the network.  \n",
    "The number of neurons in the output layer depends on the goal of the network. In our case, we need to classify 10 digits, so we are going to have 10 output neurons.  \n",
    "The output is a 10x1 vector, where each element in the vector is the output of the respective neuron. The index holding the highest output value determines the prediction of the network.  \n",
    "For example, if the highest value was in index 4, then the network classified our input as '4'.  \n",
    "We update the weights and biases for the network based on this output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent, Backpropagation and Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our network set up, we must train the network to fine tune the weights. This, hopefully, results in a higher classification accuracy.  \n",
    "**Note:** We measure accuracy as a percentage of correct classifications when the network is given some arbitrary amount of input (eg. 10000).  \n",
    "\n",
    "We train the network using Stochastic Gradient Descent (SGD), and backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def stochastic_gradient_descent(self, training_set, learning_rate, epochs, lmbda):\n",
    "        for i in range(epochs):\n",
    "            shuffle(training_set)\n",
    "            for tuple_ in training_set:\n",
    "                self.update_weights(tuple_, len(training_set), learning_rate, lmbda)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we shuffle the incoming training examples. This is done to prevent overfitting, since the network won't get the inputs in the same order in every epoch.  \n",
    "The weights are updated after each training example. This is computationally intensive on large databases like the MNIST data.  \n",
    "An improvement can be made by introducing mini-batches, where the network is trained on a small number of examples before updating the weights.\n",
    "\n",
    "The SGD minimizes a cost function, in particular, the cross entropy cost function. A cost function measures how \"far off\" the network is with its prediction. Minimizing this gives us the best change in weights/biases needed to get a more accurate prediction.\n",
    "\n",
    "The rule for updating the weights is:\n",
    "$\\large{w = w - \\alpha \\frac{\\delta C}{\\delta w}}$  \n",
    "\n",
    "The rule for updating the biases is:\n",
    "$\\large{b = b - \\alpha \\frac{\\delta C}{\\delta b}}$  \n",
    "\n",
    "where $C$ is the cost function, and $\\alpha$ is the learning rate.\n",
    "\n",
    "To find $\\frac{\\delta C}{\\delta w}$ and $\\frac{\\delta C}{\\delta b}$, we use backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "```python\n",
    "def backpropagation(self, image, label):\n",
    "\n",
    "        bias_gradients = [np.zeros(b.shape) for b in self.biases]\n",
    "        weight_gradients = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # Feed Forward\n",
    "        activations = []\n",
    "        transfers = [image]\n",
    "\n",
    "        for k in range(self.num_layers - 1):\n",
    "            activations.append(np.dot(self.weights[k], transfers[k]) + self.biases[k])\n",
    "            transfers.append(self.sigmoid(activations[k]))\n",
    "\n",
    "        last_neuron_output = transfers[-1]\n",
    "\n",
    "        # Back Propagation\n",
    "        # Calculate the correction on last neuron\n",
    "        last_neuron_error = self.cross_entropy_cost_prime(last_neuron_output, label)\n",
    "\n",
    "        bias_gradients[-1] = last_neuron_error\n",
    "        weight_gradients[-1] = np.dot(last_neuron_error, transfers[-2].transpose())\n",
    "\n",
    "        # correct the other neurons in the hidden layers based on previous calculations\n",
    "        hidden_neuron_error = last_neuron_error\n",
    "        for k in range(2, self.num_layers):\n",
    "            sp = self.sigmoid_prime(activations[-k])\n",
    "            hidden_neuron_error = np.dot(self.weights[-k+1].transpose(), hidden_neuron_error) * sp\n",
    "            bias_gradients[-k] = hidden_neuron_error\n",
    "            weight_gradients[-k] = np.dot(hidden_neuron_error, transfers[-k-1].transpose())\n",
    "\n",
    "        return (weight_gradients, bias_gradients)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onces the gradients have been calculated, we update the weights and biases using the previously metioned rules.  \n",
    "However, we can add a regularizer to the weights which helps in preventing over-fitting. This is known as the L2 regularizer, or weight decay. We scale the weights by some factor (a hyper-parameter) on every update.\n",
    "\n",
    "When added to the cross entropy cost function, the rule for updating the weights is changed to \n",
    "$w = (1 - \\frac{\\alpha\\lambda}{n})w - \\alpha \\frac{\\delta C}{\\delta w}$, where $\\lambda$ is a hyper-parameter chosen by us. Usually, the higher the number of training examples, the higher the parameter.  \n",
    "The bias update rule remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    " def update_weights(self, tuple_, training_set_size, learning_rate, lmbda):\n",
    "        # call backpropagation on each tuple_\n",
    "        image = tuple_[0]\n",
    "        label = tuple_[1]\n",
    "        weight_gradients, bias_gradients = self.backpropagation(image, label)\n",
    "\n",
    "        # update weights and biases\n",
    "        for k in range(len(self.weights)):\n",
    "            self.weights[k] = (1 - ((learning_rate * lmbda) / training_set_size)) * self.weights[k] - (learning_rate * weight_gradients[k])\n",
    "\n",
    "        for k in range(len(self.biases)):\n",
    "            self.biases[k] -= learning_rate * bias_gradients[k]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database contains 70,000 images, out of which 60,000 are used for training, and 10,000 are used for testing. However, this is still insufficient data with which to train a network.  \n",
    "\n",
    "One technique used it the $k$-fold cross validation. Here the data is split into $k$ mutually exclusive independent subsets, chosen at random.  \n",
    "We iterate $k$ times, and on every iteration, we train on everything but the $k^{th}$ subset. That subset is used for testing. The average accuracy for the test is taken to be the final accuracy for the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def k_fold_cross_validation(dataset, num_folds, layers, learning_rate, epochs, lmbda):\n",
    "    #Split dataset into k mutually exclusive subsets (folds)\n",
    "    folds = split_dataset(dataset.copy(), num_folds)\n",
    "    fold_accuracy = []\n",
    "    for i in range(num_folds):\n",
    "        print(\"\\nTraining with folds \", end=\"\")\n",
    "        training_folds = []\n",
    "\n",
    "        # Combine all folds into a single training set\n",
    "        for j in range(num_folds):\n",
    "            if (i != j):\n",
    "                print(j+1, \" \", end=\"\")\n",
    "                training_folds.append(folds[j])\n",
    "        print(\"\")\n",
    "\n",
    "        training_folds = sum(training_folds,[])\n",
    "        testing_folds = folds[i]\n",
    "\n",
    "        # initialize and train our network\n",
    "        network = Network(layers)\n",
    "        network.stochastic_gradient_descent(training_folds, learning_rate, epochs, lmbda)\n",
    "\n",
    "        # test our network\n",
    "        print(\"\\nTesting with fold\", i+1)\n",
    "        fold_accuracy.append((network.network_accuracy(testing_folds)/len(testing_folds))*100)\n",
    "        print(\"Classification Accuracy in Fold \", i+1, \": \", end=\"\")\n",
    "        print(\"%.3f%%\" % fold_accuracy[i])\n",
    "        print(\"________________________________________\")\n",
    "\n",
    "    return fold_accuracy\n",
    "\n",
    "def split_dataset(dataset, num_folds):\n",
    "    folds = []\n",
    "    fold_size = int(len(dataset)/num_folds)\n",
    "    for i in range(num_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset))\n",
    "            fold.append(dataset.pop(index))\n",
    "        folds.append(fold)\n",
    "\n",
    "    return folds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following hyper-parameters to test varying hidden layer sizes:\n",
    "\n",
    "```python\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "lmbda = 2.0\n",
    "num_folds = 5\n",
    "```\n",
    "\n",
    "A few notes:\n",
    "\n",
    "1. Too many hidden layers can cause the network to start learning slowly, and it might take longer to get to an optimal accuracy for the algorithm. We see this with 5 hidden layers, with 30 neurons each.\n",
    "\n",
    "2. Too few hidden neurons, regardless of the number of hidden layers, may cause the network to not learn enough about the data. Therefore there will be poor prediction accuracy.\n",
    "\n",
    "3. At a certain point, we do more computation (in turn take longer) for very little improvement in accuracy. We can see this when increase gradually from 15 to 75 neurons with a single hidden layer. The accuracy seems to plateau, however, the rate of improvement might increase.\n",
    "\n",
    "4. Our network has a sweet spot: 1 hidden layer, 17-35 neurons. This way, we do not wait long to train, and we get consistent optimal result. (Empiracally, we've seen our network max out at 95-96% accuracy).\n",
    "\n",
    "5. The hyper-parameters need to be tested for an even more optimal accuracy.\n",
    "\n",
    "TO-DO: pretty sure we need to square # neurons in this table \n",
    "\n",
    "| Layers |   Neurons    | Network Accuracy | \n",
    "| ---------- | ------------- | ------------- |\n",
    "| 1 | 2  |36.920%          \n",
    "| 1 | 5  |77.590%            \n",
    "| 1 | 15 | 92.780%           \n",
    "| 1 | 30 | 94.720%           \n",
    "| 1 | 45 | 95.930%           \n",
    "| 1 | 75 | 96.710%\n",
    "| 2 |  5 | 74.510%           \n",
    "| 2 | 30 | 95.250%\n",
    "| 5 | 30 | 94.750% \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a feed forward RBF neural network in python that classifies the images found in the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feed forward Radial Basis Function neural Network (RBFN) is a 3-layer neural network. It consists of a input layer, a single hidden layer, and an output layer.\n",
    "\n",
    "Note that while this is a fully connected network, we do not use weights between the input and hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the Feed Forward neural network, the input layer has 784 neurons, each representing the normalized greyscale values.\n",
    "\n",
    "The input (a 784x1 vector) is given to each hidden layer neuron, without any weights, or weighted sums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neuron in this layer is also known as a RBF neuron. Each RBF neuron stores a prototype vector. An input is compared to this vector via the means of a gaussian function. Each neuron has a unique gaussian activation function, centered at the prototype vector.\n",
    "\n",
    "The number of neurons in this layer can be determined by various methods. One such method is k-means clustering (discussed later). This method also gives a prototype vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer outputs a \"score\" which is calculated by taking a weighted sum of the activation values from the hidden layer. The number of neurons in this layer is determined by the number of things we need to classify, in our case - 10 neurons.\n",
    "\n",
    "We get a 10x1 vector as the output of the network, where the index with the highest value is our classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is a way to determine the prototype vectors for the hidden RBF layer. By extension, it also determines the gaussian activation functions.\n",
    "\n",
    "It works by randomly selecting $k$ points (centroids) from our data, clustering near-by points around the selection using the euclidean distance (clusters), and updating the centroids to the mean of their respective clusters.  \n",
    "We keep doing this in a loop, using one of the following as our stopping condition:\n",
    "1. No new point is assigned to any cluster (i.e. the centroids don't move)\n",
    "2. For some maximum number of iterations\n",
    "3. If the centroids don't update/move significantly determined by an arbitrary threshold.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# create clusters and update centroids till optimal centroids found\n",
    "def k_means_clustering(k, data, iterations):\n",
    "\n",
    "    # initialize centroids and starting clusters\n",
    "    centroids = init_centroids(k, data)\n",
    "    clusters = create_clusters(data, centroids)\n",
    "\n",
    "    # loop update->create->update till no more change can be made to the centroids\n",
    "    old_difference = 0\n",
    "    i = 0\n",
    "    while i < iterations:\n",
    "        old_centroids = centroids\n",
    "        centroids = update_centroids(clusters)\n",
    "        clusters = create_clusters(data, centroids)\n",
    "\n",
    "        # find out how much our centroids moved\n",
    "        num_centroids = len(centroids)\n",
    "        differences = []\n",
    "        for i in range(num_centroids):\n",
    "            differences.append(np.linalg.norm(old_centroids[i] - centroids[i]))\n",
    "\n",
    "        max_diff = max(differences)\n",
    "\n",
    "        difference_change = abs((max_diff-old_difference)/np.mean([old_difference,max_diff])) * 100\n",
    "\n",
    "        if np.isnan(difference_change):\n",
    "            break\n",
    "        i+=1\n",
    "\n",
    "    return (centroids,clusters)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial centroids are chosen randomly from our data. Note that we have a maximum number of iterations if the algorithm takes too long to converge. An empirical experience seems to suggest that with 70000 datapoints, it can take anywhere from 20-100 iterations to converge depending on the number of clusters/centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def gaussian(self, x, mu, cluster):\n",
    "        # calculate beta value\n",
    "        sigma = 0.0\n",
    "        for i in range(len(cluster)):\n",
    "            sigma += np.linalg.norm(x - mu)\n",
    "\n",
    "        sigma = sigma * (1.0/len(cluster))\n",
    "\n",
    "        beta = 1.0 / (2*sigma**2)\n",
    "\n",
    "        # calculate actual gaussian\n",
    "        phi = np.exp(-beta*(np.linalg.norm(x - mu)**2))\n",
    "\n",
    "        return phi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaussian activation function more commonly used is \n",
    "<center>$\\large{\\mathrm{e}^{-\\beta(\\|x - mu\\|^{2})}}$  \n",
    "\n",
    "To propagate the input forward, we calculate the gaussian activations for all RBF neurons, and then take a weighted sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy results for our RBF network are too low to be meaningful (10-15%). A reason for this could be, the calculation for the $\\beta$ values was taking too long so we could only use a 1000 datapoints for a pragmatic runtime.\n",
    "This probably led to underfitting because of the very small amount of training datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Citations\n",
    "\n",
    "1. A List of Cost Functions Used in Neural Networks, alongside Applications, stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications.  \n",
    "\n",
    "2. Brownlee , Jason. machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/.  \n",
    "\n",
    "3. Dobrzanski, Michael Daniel. Github, github.com/MichalDanielDobrzanski/DeepLearningPython35.  \n",
    "\n",
    "4. Loeber, John. K-Means Clustering on Handwritten Digits, johnloeber.com/docs/kmeans.html.  \n",
    "\n",
    "5. Mccormick, Chris. “Radial Basis Function Network (RBFN) Tutorial.” Radial Basis Function Network (RBFN) Tutorial · Chris McCormick, mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/.  \n",
    "\n",
    "6. Nielsen , Michael. Neural Networks and Deep Learning, neuralnetworksanddeeplearning.com/.  \n",
    "\n",
    "7. Roelants, Peter. “How to Implement a Neural Network Intermezzo 1.” Peter's Notes, peterroelants.github.io/posts/neural_network_implementation_intermezzo01/.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
